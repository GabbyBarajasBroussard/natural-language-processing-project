{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import acquire\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import os\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "\n",
    "from env import github_token, github_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "def make_soup(url):\n",
    "    '''\n",
    "    This function takes in a url and requests and parses HTML\n",
    "    returning a soup object which holds the html of the site as a text.\n",
    "    '''\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    response = get(site, headers=headers)\n",
    "    \n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################################################\n",
    "def get_github_urls():\n",
    "    '''\n",
    "    This function scrapes all of the urls from\n",
    "    the github search page for animal crossing and returns a list of urls.\n",
    "    '''\n",
    "    urls = []\n",
    "    for i in range(1,101):\n",
    "        \n",
    "        url = f'https://github.com/search?p={i}&q=animal+crossing&type=Repositories'\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_links():\n",
    "    '''\n",
    "    This function scrapes all of the individual urls from\n",
    "    the github first search page and returns a list of urls.\n",
    "    '''\n",
    "    repos = []\n",
    "    for i in range(0,29):\n",
    "        link_suffix = soup.find_all('a', itemprop='name codeRepository')[i].get('href')\n",
    "        link  = f'https://github.com/{link_suffix}'\n",
    "        repos.append(link)\n",
    "    return repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_repo_links():\n",
    "    \"\"\"\n",
    "    This function takes a list of urls, parses the html of the page, retrieves the links of all of the repo sites on the page\n",
    "    and returns an appended list of all the repository links.\n",
    "    \"\"\"\n",
    "    repo_links = []\n",
    "    \n",
    "    urls = get_githubpgs()\n",
    "    \n",
    "    for url in urls:\n",
    "        soup = souper(url)\n",
    "        \n",
    "        repos = get_repo_links()\n",
    "        repo_links.append(repos)\n",
    "    \n",
    "    return repo_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
