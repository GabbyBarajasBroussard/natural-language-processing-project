{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "###########################################################################################################################################################################\n",
    "def basic_prep_ac():\n",
    "    df= pd.read_csv('animal_crossing.csv', index_col=0)\n",
    "    #finding the length of the readme\n",
    "    df['readme_length'] = wrangle.find_readme_length()\n",
    "    #removing the nulls\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "###########################################################################################################################################################################\n",
    "def clean(text):\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') \n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "             .encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    # Return a joined string\n",
    "    return \" \".join([wnl.lemmatize(word) for word in words if word not in stopwords])\n",
    "###########################################################################################################################################################################\n",
    "def split(df, stratify_by=None):\n",
    "    \"\"\"\n",
    "    3 way split for train, validate, and test datasets\n",
    "    To stratify, send in a column name\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(df, test_size=.2, random_state=123, stratify=df[stratify_by])\n",
    "    \n",
    "    train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train[stratify_by])\n",
    "    \n",
    "    return train, validate, test\n",
    "###########################################################################################################################################################################\n",
    "def basic_clean(text):\n",
    "    '''\n",
    "    This function takes in a string and normalizes it by lowercasing\n",
    "    everything and replacing anything that is not a letter, number, \n",
    "    whitespace or a single quote.\n",
    "    '''\n",
    "    \n",
    "    #lowercase all letters in the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # normalize unicode by encoding into ASCII (ignore non-ASCII characters)\n",
    "    # then decoding back into unicode \n",
    "    text = unicodedata.normalize('NFKD', text)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "\n",
    "    # remove any that is not a letter, number, single quote, or whitespace\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "###########################################################################################################################################################################\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    This function takes in a string and returns the string will the\n",
    "    words tokenized\n",
    "    '''\n",
    "\n",
    "    # Create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "    # Use the tokenizer\n",
    "    text = tokenizer.tokenize(text, return_str=True)\n",
    "    \n",
    "    return text   \n",
    "\n",
    "\n",
    "###########################################################################################################################################################################\n",
    "\n",
    "def stem(text):\n",
    "    '''\n",
    "    This function takes in a string and returns the string after applying\n",
    "    stemming to all the words.\n",
    "    '''\n",
    "\n",
    "    # Create the porter stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "    # Apply the stemmer to each word in our string.\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again\n",
    "    text_stemmed = ' '.join(stems)\n",
    "\n",
    "    return text_stemmed\n",
    "\n",
    "\n",
    "###########################################################################################################################################################################\n",
    "\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    This function takes in a string and returns the string after applying\n",
    "    lemmatization to all the words.\n",
    "    '''\n",
    "\n",
    "    # Create the Lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in text.split()]\n",
    "\n",
    "    # Join our list of words into a string again; assign to a variable to save changes.\n",
    "    text_lemmatized = ' '.join(lemmas)\n",
    "    \n",
    "    return text_lemmatized\n",
    "\n",
    "###########################################################################################################################################################################\n",
    "\n",
    "\n",
    "def remove_stopwords(text, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function takes in a string and optional lists of extra_words and \n",
    "    words to exclude from the list and then returns the string after removing stop_words\n",
    "    '''\n",
    "\n",
    "    # Define the stop word list\n",
    "    stopword_list = stopwords.words('english')\n",
    "\n",
    "    # add extra_words (if any) to the stopwords list\n",
    "    if len(extra_words) > 0:\n",
    "        stopword_list = stopword_list.append(extra_words)\n",
    "      \n",
    "    # remove exclude_words (if any) from the stopwords list\n",
    "    if len(exclude_words) > 0:\n",
    "        stopword_list = stopword_list.remove(exclude_words)   \n",
    "\n",
    "    # Split words in text.\n",
    "    text = text.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in text if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings; assign to a variable to keep changes.\n",
    "    text_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "    return text_without_stopwords\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################################################################################################\n",
    "\n",
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\\\n",
    "                            .apply(lemmatize)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean).apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean).apply(lemmatize)\n",
    "    \n",
    "    return df[['title', column, 'stemmed', 'lemmatized', 'clean']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
